{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11e9d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from multiprocessing.dummy import Pool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from skimage.morphology import binary_opening, disk, label\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from  torchvision.transforms import *\n",
    "import torchvision.transforms as transforms\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171031b",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h1  style=\"padding:20px;color:white;margin:0;font-size:200%;text-align:left;display:fill;border-radius:15px;background-color:\t#1E90FF;overflow:hidden\"><center> Airbus Ship Detection Challenge </center></h1>\n",
    "    \n",
    "<img src = \"https://www.mdpi.com/sensors/sensors-22-02713/article_deploy/html/images/sensors-22-02713-g006.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120499f",
   "metadata": {},
   "source": [
    "> ## [**1. Load Data**](#title-one)\n",
    " >## [**2. Model Unet**](#title-two)\n",
    " >## [**3. Submission**](#title-three)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902676b",
   "metadata": {},
   "source": [
    "<a id='title-one'></a>\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h1  style=\"padding:20px;color:white;margin:0;font-size:200%;text-align:left;display:fill;border-radius:15px;background-color:\t#1E90FF;overflow:hidden\"> Load Data</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c181d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_mask(mask, shape=(768, 768)):\n",
    "    pixels = mask.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] +1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    if not isinstance(mask_rle, str):\n",
    "        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "        return img.reshape(shape).T\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    ends = starts + lengths\n",
    "    im = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        im[lo:hi] = 1\n",
    "    return im.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c6f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class param:\n",
    "    img_size = (768, 768)\n",
    "    bs = 8\n",
    "    num_workers = 4\n",
    "    lr = 0.001\n",
    "    epochs = 100\n",
    "    unet_depth = 5\n",
    "    unet_start_filters = 8\n",
    "    log_interval = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2beda0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './'\n",
    "TRAIN = 'train_v2/'\n",
    "TEST = 'test_v2/'\n",
    "SEGMENTATION = 'train_ship_segmentations_v2.csv'\n",
    "DETECTION_TEST_PRED = 'ship_detection.csv'\n",
    "train_dpath = 'train_v2/'\n",
    "anno_path = 'train_ship_segmentation_v2.csv'\n",
    "excluded_filenames = ['6384c3e78.jpg', ]\n",
    "original_img_size = (768, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c58ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "annos = pd.read_csv('train_ship_segmentations_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e16273",
   "metadata": {},
   "outputs": [],
   "source": [
    "annos['EncodedPixels_flag'] = annos['EncodedPixels'].map(lambda x: 1 if isinstance(x,str) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1040f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = annos.groupby('ImageId').agg({'EncodedPixels_flag':'sum'}).reset_index().rename(columns={'EncodedPixels_flag':'ships'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f70156a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_w_ships = imgs[imgs['ships'] > 0]\n",
    "imgs_wo_ships = imgs[imgs['ships'] == 0].sample(10_000,random_state=69278)\n",
    "\n",
    "selected_imgs = pd.concat([imgs_w_ships,imgs_wo_ships])\n",
    "selected_imgs['has_ships'] = selected_imgs['ships'] >0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05326f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, val_imgs = train_test_split(selected_imgs,test_size=0.01,stratify=selected_imgs['has_ships'],random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1dfc3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train = 44672 val  = 7884 \n"
     ]
    }
   ],
   "source": [
    "train_imgs, val_imgs = train_test_split(selected_imgs,test_size=0.15,stratify=selected_imgs['has_ships'],random_state=123) \n",
    "train_fnames = train_imgs['ImageId'].values\n",
    "val_fnames = val_imgs['ImageId'].values\n",
    "test_fnames = pd.read_csv('sample_submission_v2.csv')['ImageId'].values\n",
    "#p ,train_fnames =  train_test_split(train_fnames,test_size=0.1,random_state=123)\n",
    "#p_1,val_fnames =  train_test_split(val_fnames,test_size=0.1,random_state=123)\n",
    "print(f\"\"\"Train = {len(train_fnames)} val  = {len(val_fnames)} \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd5c897b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>ships</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_ships</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>8500</td>\n",
       "      <td>8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>36172</td>\n",
       "      <td>36172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ImageId  ships\n",
       "has_ships                \n",
       "False         8500   8500\n",
       "True         36172  36172"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs.groupby('has_ships').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8f0ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 img_dpath,\n",
    "                 img_fnames,\n",
    "                 img_transform,\n",
    "                 mask_encodings=None,\n",
    "                 mask_size=None,\n",
    "                 mask_transform=None):\n",
    "        self.img_dpath = img_dpath\n",
    "        self.img_fnames = img_fnames\n",
    "        self.img_transform = img_transform\n",
    "\n",
    "        self.mask_encodings = mask_encodings\n",
    "        self.mask_size = mask_size\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # https://github.com/pytorch/vision/issues/9#issuecomment-304224800\n",
    "        seed = np.random.randint(2147483647)\n",
    "\n",
    "        fname = self.img_fnames[i]\n",
    "        fpath = os.path.join(self.img_dpath, fname)\n",
    "        img = Image.open(fpath)\n",
    "        if self.img_transform is not None:\n",
    "            random.seed(seed)\n",
    "            img = self.img_transform(img)\n",
    "\n",
    "        if self.mask_encodings is None:\n",
    "            img =  self.img_transform(Image.open(fpath))\n",
    "            return img, 0,self.img_fnames[i]\n",
    "\n",
    "        if self.mask_size is None or self.mask_transform is None:\n",
    "            raise ValueError('If mask_dpath is not None, mask_size and mask_transform must not be None.')\n",
    "\n",
    "        mask = np.zeros(self.mask_size, dtype=np.uint8)\n",
    "        if self.mask_encodings[fname][0] == self.mask_encodings[fname][0]: # NaN doesn't equal to itself\n",
    "            for encoding in self.mask_encodings[fname]:\n",
    "                mask += rle_decode(encoding, self.mask_size)\n",
    "        mask = np.clip(mask, 0, 1)\n",
    "\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        random.seed(seed)\n",
    "        mask = self.mask_transform(mask)\n",
    "        img = self.img_transform(Image.open(fpath))\n",
    "        return  img, torch.from_numpy(np.array(mask, dtype=np.int64)),self.img_fnames[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e0f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_encodings(annos,fnames):\n",
    "    a = annos[annos['ImageId'].isin(fnames)]\n",
    "    return a.groupby('ImageId')['EncodedPixels'].apply(lambda x: x.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8b69db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = transforms.Compose([\n",
    "                                 ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),transforms.Resize(param.img_size),transforms.ToTensor()])\n",
    "val_tfms = transforms.Compose([\n",
    "                               ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),transforms.Resize(param.img_size),transforms.ToTensor()])\n",
    "test_tfms =transforms.Compose([\n",
    "                               ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),transforms.ToTensor()])\n",
    "mask_tfms = transforms.Compose([transforms.Resize(param.img_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddabe8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImgDataset(train_dpath, train_fnames, train_tfms, get_mask_encodings(annos, train_fnames), original_img_size, mask_tfms)\n",
    "val_ds = ImgDataset(train_dpath, val_fnames, val_tfms, get_mask_encodings(annos, val_fnames), original_img_size, mask_tfms)\n",
    "test_ds = ImgDataset(TEST, test_fnames, val_tfms, None, original_img_size, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8dbf1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,\n",
    "                      batch_size=param.bs,num_workers=2,\n",
    "                      shuffle=True)\n",
    "val_dl = DataLoader(val_ds,\n",
    "                    batch_size=param.bs,num_workers=2,\n",
    "                    shuffle=False)\n",
    "test_dl = DataLoader(test_ds,\n",
    "                    batch_size=5,num_workers=2,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2cb97c",
   "metadata": {},
   "source": [
    "<a id='title-two'></a>\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h1  style=\"padding:20px;color:white;margin:0;font-size:200%;text-align:left;display:fill;border-radius:15px;background-color:\t#1E90FF;overflow:hidden\">Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c02cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        \n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p))\n",
    "    \n",
    "class SaveFeatures():\n",
    "    \n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()\n",
    "        \n",
    "        \n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, rn):\n",
    "        super().__init__()\n",
    "        self.rn = rn\n",
    "        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.rn(x))\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x)\n",
    "        return x[:,0]\n",
    "    \n",
    "    def close(self):\n",
    "        print(1)\n",
    "        for sf in self.sfs: sf.remove()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfc47981",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = resnet34\n",
    "cut = model_meta[arch]['cut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "253318a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cut': -2,\n",
       " 'split': <function fastai.vision.learner._resnet_split(m)>,\n",
       " 'stats': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_meta[arch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63247bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base():                   #load ResNet34 model\n",
    "    layers = cut_model(arch(True), cut)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def load_pretrained(model): #load a model pretrained on ship/no-ship classification\n",
    "    \n",
    "    weights = torch.load('resnet34-333f7ec4.pth', map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cd25c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "rn = load_pretrained(get_base())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f6157a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_ds[0][0].reshape(1,3,768,768)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2bae8973",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc5eb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1383981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()\n",
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ca76cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2.0 * (pred*targs).sum() / ((pred+targs).sum() + 1.0)\n",
    "\n",
    "def IoU(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    intersection = (pred*targs).sum()\n",
    "    return intersection / ((pred+targs).sum() - intersection + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c63512fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.param"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bea64994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t up1.x_conv.weight\n",
      "\t up1.x_conv.bias\n",
      "\t up1.tr_conv.weight\n",
      "\t up1.tr_conv.bias\n",
      "\t up1.bn.weight\n",
      "\t up1.bn.bias\n",
      "\t up2.x_conv.weight\n",
      "\t up2.x_conv.bias\n",
      "\t up2.tr_conv.weight\n",
      "\t up2.tr_conv.bias\n",
      "\t up2.bn.weight\n",
      "\t up2.bn.bias\n",
      "\t up3.x_conv.weight\n",
      "\t up3.x_conv.bias\n",
      "\t up3.tr_conv.weight\n",
      "\t up3.tr_conv.bias\n",
      "\t up3.bn.weight\n",
      "\t up3.bn.bias\n",
      "\t up4.x_conv.weight\n",
      "\t up4.x_conv.bias\n",
      "\t up4.tr_conv.weight\n",
      "\t up4.tr_conv.bias\n",
      "\t up4.bn.weight\n",
      "\t up4.bn.bias\n",
      "\t up5.weight\n",
      "\t up5.bias\n"
     ]
    }
   ],
   "source": [
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True and 'rn.' not in name:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "optim = torch.optim.Adam(params_to_update, lr=0.01,weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f0878a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63fa185354a43ff846ce5a0454b7f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,(X,y,_) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dl):\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m      9\u001b[0m         optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m         X ,y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(dev),y\u001b[38;5;241m.\u001b[39mto(dev)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "dev = 'cuda'\n",
    "model = model.to(dev)\n",
    "loss = MixedLoss(10.0, 2.0)\n",
    "\n",
    "for epoch in tqdm(range(5)):\n",
    "    for i,(X,y,_) in enumerate(train_dl):\n",
    "        optim.zero_grad()\n",
    "        X ,y = X.to(dev),y.to(dev)\n",
    "        pred = model(X)\n",
    "        l = loss(pred,y)\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        if i %100 == 0:\n",
    "            print(l.item())\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89c3719e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_item = 6\n",
    "#names\n",
    "mask, predicted = y[num_item].to('cpu'),torch.where(output[num_item]>0.5,torch.tensor(1),torch.tensor(0))\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.subplot(133)\n",
    "plt.imshow(predicted.detach().cpu().reshape(768,768), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aa56c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'unet_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d22ce3e",
   "metadata": {},
   "source": [
    "<a id='title-three'></a>\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h1  style=\"padding:20px;color:white;margin:0;font-size:200%;text-align:left;display:fill;border-radius:15px;background-color:\t#1E90FF;overflow:hidden\"> Submission</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b37a1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Unet(rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed94b855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load('unet_final'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac28ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "dev = 'cuda'\n",
    "model2 = model2.to(dev)\n",
    "final = pd.DataFrame()\n",
    "for i, (X, _,name) in enumerate(test_dl):\n",
    "    tmp = pd.DataFrame()\n",
    "    X = X.to('cuda')\n",
    "    output = model2(X)\n",
    "    a = torch.sigmoid(output)\n",
    "    code = [decode_mask(torch.where(x>0.6,torch.tensor(1),torch.tensor(0)).to('cpu').int()) for x in a]\n",
    "    tmp['ImageId'] = name\n",
    "    tmp['EncodedPixels'] = code\n",
    "    final = pd.concat([final,tmp],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae845183",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ad04ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('prob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb163321",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = df1[df1['ship']<0.5]['ImageId'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5049f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48f7c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['ImageId'].isin(img),'EncodedPixels'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "674adfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454c7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
